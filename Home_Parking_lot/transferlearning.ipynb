{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download labeling from GitHub - https://github.com/tzutalin/labelImg\n",
    "\n",
    "\n",
    "`!pip install pyqt5`\n",
    "\n",
    "`!pip install lxml`\n",
    "\n",
    "\n",
    "Installation guide - https://github.com/heartexlabs/labelImg#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command in the terminal\n",
    "\n",
    "`pyrcc5 -o libs/resources.py resources.qrc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Use a pre-trained model\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Change the number of classes\n",
    "num_classes = 2  # 1 class (person) + background\n",
    "model.head.classification_head.cls_logits = nn.Conv2d(256, num_classes, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Send the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform if transform else transforms.ToTensor()\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"images\"))))\n",
    "        self.labels = list(sorted(os.listdir(os.path.join(root_dir, \"labels\"))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, \"images\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.root_dir, \"labels\", self.labels[idx])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "        # Apply transformation after getting original size\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # Read YOLO label file\n",
    "        with open(label_path, \"r\") as file:\n",
    "            lines = file.read().splitlines()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "            labels.append(int(class_id))\n",
    "\n",
    "            x_min = img_width * (x_center - width / 2)\n",
    "            y_min = img_height * (y_center - height / 2)\n",
    "            x_max = img_width * (x_center + width / 2)\n",
    "            y_max = img_height * (y_center + height / 2)\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.tensor(boxes, dtype=torch.float32)\n",
    "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# Define your own paths here\n",
    "train_dataset = YOLODataset(\"data/train set\")\n",
    "valid_dataset = YOLODataset(\"data/validation set\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug output:\n",
      "Boxes: tensor([[1.5000e+01, 5.0500e+02, 5.4000e+01, 5.4100e+02],\n",
      "        [5.2001e+01, 4.9600e+02, 8.8001e+01, 5.3400e+02],\n",
      "        [1.2100e+02, 4.8000e+02, 1.5800e+02, 5.1800e+02],\n",
      "        [1.5200e+02, 4.7500e+02, 1.8900e+02, 5.1000e+02],\n",
      "        [1.8500e+02, 4.7200e+02, 2.2100e+02, 5.0300e+02],\n",
      "        [2.1500e+02, 4.6600e+02, 2.5300e+02, 4.9600e+02],\n",
      "        [2.4900e+02, 4.5900e+02, 2.8100e+02, 4.8800e+02],\n",
      "        [2.7500e+02, 4.5200e+02, 3.1000e+02, 4.8000e+02],\n",
      "        [4.5500e+02, 3.7700e+02, 4.8700e+02, 4.0200e+02],\n",
      "        [4.8500e+02, 3.6400e+02, 5.2400e+02, 3.8500e+02],\n",
      "        [4.6600e+02, 4.1500e+02, 5.0300e+02, 4.4400e+02],\n",
      "        [1.0010e+00, 5.7000e+02, 3.5000e+01, 6.0600e+02],\n",
      "        [3.7000e+01, 5.6000e+02, 7.6001e+01, 5.9800e+02],\n",
      "        [7.8001e+01, 5.5400e+02, 1.1200e+02, 5.8800e+02],\n",
      "        [1.1200e+02, 5.3900e+02, 1.5400e+02, 5.7900e+02],\n",
      "        [1.4900e+02, 5.3100e+02, 1.9000e+02, 5.6500e+02],\n",
      "        [1.8800e+02, 5.2200e+02, 2.2400e+02, 5.5400e+02],\n",
      "        [2.2500e+02, 5.1700e+02, 2.6200e+02, 5.4800e+02],\n",
      "        [2.5500e+02, 5.0300e+02, 2.9700e+02, 5.3900e+02],\n",
      "        [3.1900e+02, 4.9200e+02, 3.6200e+02, 5.2400e+02],\n",
      "        [3.5300e+02, 4.8700e+02, 3.9100e+02, 5.1400e+02],\n",
      "        [3.3000e+02, 5.7500e+02, 3.7500e+02, 6.1200e+02],\n",
      "        [2.9000e+02, 5.8600e+02, 3.3800e+02, 6.2900e+02],\n",
      "        [2.5200e+02, 5.9500e+02, 3.0000e+02, 6.3700e+02],\n",
      "        [2.0600e+02, 6.0500e+02, 2.5200e+02, 6.4600e+02],\n",
      "        [1.6200e+02, 6.1800e+02, 2.1200e+02, 6.6300e+02],\n",
      "        [1.1900e+02, 6.3000e+02, 1.6600e+02, 6.7700e+02],\n",
      "        [1.4000e+02, 6.6600e+02, 1.9100e+02, 7.1700e+02],\n",
      "        [7.6000e+01, 6.4900e+02, 1.1900e+02, 6.8800e+02],\n",
      "        [1.9999e+01, 6.6100e+02, 6.7000e+01, 7.0200e+02],\n",
      "        [4.3700e+02, 5.7500e+02, 4.8800e+02, 6.1900e+02],\n",
      "        [4.6400e+02, 6.4800e+02, 5.2600e+02, 7.0200e+02],\n",
      "        [2.0800e+02, 5.4100e+02, 2.5500e+02, 5.8200e+02],\n",
      "        [1.7400e+02, 5.5600e+02, 2.1300e+02, 5.9500e+02],\n",
      "        [1.3600e+02, 5.6400e+02, 1.7500e+02, 6.0500e+02],\n",
      "        [8.7001e+01, 5.7400e+02, 1.3300e+02, 6.1700e+02],\n",
      "        [4.9000e+01, 5.8100e+02, 8.9000e+01, 6.2700e+02],\n",
      "        [4.9997e+00, 5.9800e+02, 4.5000e+01, 6.4000e+02],\n",
      "        [4.4000e+02, 4.8700e+02, 4.8400e+02, 5.2000e+02],\n",
      "        [4.6700e+02, 4.7500e+02, 5.1100e+02, 5.1100e+02],\n",
      "        [4.3700e+02, 4.6200e+02, 4.7900e+02, 4.8600e+02],\n",
      "        [5.1300e+02, 4.4500e+02, 5.5300e+02, 4.7300e+02],\n",
      "        [5.4300e+02, 4.5500e+02, 5.9100e+02, 4.9000e+02],\n",
      "        [5.2700e+02, 3.8500e+02, 5.6700e+02, 4.1200e+02],\n",
      "        [5.2400e+02, 3.4600e+02, 5.6400e+02, 3.6300e+02],\n",
      "        [5.6900e+02, 3.5800e+02, 6.0900e+02, 3.7700e+02],\n",
      "        [5.9400e+02, 3.4800e+02, 6.2700e+02, 3.6600e+02],\n",
      "        [5.7700e+02, 3.8900e+02, 6.1700e+02, 4.2000e+02],\n",
      "        [6.0900e+02, 4.2300e+02, 6.4900e+02, 4.4500e+02],\n",
      "        [6.2000e+02, 3.8900e+02, 6.5700e+02, 4.0900e+02],\n",
      "        [6.3000e+02, 3.7100e+02, 6.6300e+02, 3.8900e+02],\n",
      "        [6.5300e+02, 3.3100e+02, 6.8700e+02, 3.4900e+02],\n",
      "        [6.6200e+02, 3.6000e+02, 7.0000e+02, 3.7700e+02],\n",
      "        [6.6000e+02, 4.7500e+02, 7.1200e+02, 5.0400e+02],\n",
      "        [6.8400e+02, 5.0300e+02, 7.3600e+02, 5.3800e+02],\n",
      "        [7.1000e+02, 4.6200e+02, 7.5800e+02, 4.9000e+02],\n",
      "        [6.8800e+02, 4.2500e+02, 7.2800e+02, 4.5000e+02],\n",
      "        [6.7600e+02, 4.0800e+02, 7.1800e+02, 4.2600e+02],\n",
      "        [7.2900e+02, 3.9000e+02, 7.7400e+02, 4.1200e+02],\n",
      "        [7.7800e+02, 4.4600e+02, 8.2800e+02, 4.7100e+02],\n",
      "        [7.4200e+02, 5.4900e+02, 8.0600e+02, 5.9300e+02],\n",
      "        [8.8900e+02, 4.3500e+02, 9.4400e+02, 4.6700e+02],\n",
      "        [9.4100e+02, 4.0600e+02, 9.8200e+02, 4.2900e+02],\n",
      "        [9.2100e+02, 4.0100e+02, 9.6100e+02, 4.2400e+02],\n",
      "        [8.9700e+02, 3.9300e+02, 9.3900e+02, 4.1800e+02],\n",
      "        [8.7500e+02, 3.8800e+02, 9.1900e+02, 4.1100e+02],\n",
      "        [8.6100e+02, 3.8100e+02, 8.9800e+02, 4.0400e+02],\n",
      "        [7.6200e+02, 3.5800e+02, 7.9200e+02, 3.7600e+02],\n",
      "        [7.3000e+02, 3.4800e+02, 7.6400e+02, 3.6900e+02],\n",
      "        [1.0780e+03, 4.3600e+02, 1.1310e+03, 4.7300e+02],\n",
      "        [6.5100e+02, 5.8500e+02, 7.2000e+02, 6.2600e+02],\n",
      "        [6.7000e+02, 6.2500e+02, 7.4300e+02, 6.8300e+02],\n",
      "        [6.1000e+02, 6.0000e+02, 6.7600e+02, 6.4400e+02],\n",
      "        [5.9600e+02, 6.5800e+02, 6.6500e+02, 7.1300e+02],\n",
      "        [5.1300e+02, 6.8800e+02, 5.7700e+02, 7.2000e+02],\n",
      "        [6.0800e+02, 3.1400e+02, 6.3500e+02, 3.3100e+02]])\n",
      "Labels: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4])\n",
      "Boxes: tensor([[1.5000e+01, 5.0500e+02, 5.1000e+01, 5.4200e+02],\n",
      "        [5.1000e+01, 4.9700e+02, 8.8000e+01, 5.3600e+02],\n",
      "        [8.9000e+01, 4.9700e+02, 1.2500e+02, 5.2900e+02],\n",
      "        [1.2200e+02, 4.8100e+02, 1.5800e+02, 5.1600e+02],\n",
      "        [1.5100e+02, 4.7700e+02, 1.8800e+02, 5.1000e+02],\n",
      "        [1.8500e+02, 4.7400e+02, 2.2100e+02, 5.0400e+02],\n",
      "        [2.1300e+02, 4.6700e+02, 2.5100e+02, 4.9700e+02],\n",
      "        [2.4700e+02, 4.6100e+02, 2.7900e+02, 4.8900e+02],\n",
      "        [2.7500e+02, 4.5300e+02, 3.1000e+02, 4.8200e+02],\n",
      "        [3.2700e+02, 4.4000e+02, 3.6600e+02, 4.7000e+02],\n",
      "        [1.0003e+00, 5.7000e+02, 3.2001e+01, 6.0300e+02],\n",
      "        [3.4000e+01, 5.6200e+02, 7.3000e+01, 5.9600e+02],\n",
      "        [7.4000e+01, 5.5300e+02, 1.1200e+02, 5.8700e+02],\n",
      "        [1.1100e+02, 5.4100e+02, 1.5500e+02, 5.7800e+02],\n",
      "        [1.8600e+02, 5.2300e+02, 2.2300e+02, 5.5400e+02],\n",
      "        [2.2400e+02, 5.1800e+02, 2.6000e+02, 5.5000e+02],\n",
      "        [2.5300e+02, 5.0600e+02, 2.9700e+02, 5.3900e+02],\n",
      "        [2.9000e+02, 5.0300e+02, 3.2800e+02, 5.3200e+02],\n",
      "        [3.2100e+02, 4.9500e+02, 3.6200e+02, 5.2400e+02],\n",
      "        [3.5200e+02, 4.8900e+02, 3.9100e+02, 5.1700e+02],\n",
      "        [3.8300e+02, 4.8000e+02, 4.2200e+02, 5.0800e+02],\n",
      "        [4.1000e+02, 4.7100e+02, 4.4800e+02, 4.9500e+02],\n",
      "        [4.3900e+02, 4.6500e+02, 4.7500e+02, 4.9000e+02],\n",
      "        [4.6700e+02, 4.7600e+02, 5.0800e+02, 5.1000e+02],\n",
      "        [4.3600e+02, 4.8900e+02, 4.8000e+02, 5.2000e+02],\n",
      "        [6.0006e+00, 5.9800e+02, 4.6001e+01, 6.4200e+02],\n",
      "        [5.0000e+01, 5.8200e+02, 8.9000e+01, 6.2600e+02],\n",
      "        [8.9999e+01, 5.7400e+02, 1.3300e+02, 6.1900e+02],\n",
      "        [1.3200e+02, 5.6500e+02, 1.7300e+02, 6.0800e+02],\n",
      "        [1.7200e+02, 5.5500e+02, 2.1400e+02, 5.9400e+02],\n",
      "        [2.1000e+02, 5.4300e+02, 2.5400e+02, 5.8300e+02],\n",
      "        [3.2900e+02, 5.7500e+02, 3.7400e+02, 6.1200e+02],\n",
      "        [2.8800e+02, 5.8700e+02, 3.3600e+02, 6.2700e+02],\n",
      "        [2.5300e+02, 5.9600e+02, 2.9700e+02, 6.3600e+02],\n",
      "        [2.0600e+02, 6.0600e+02, 2.5100e+02, 6.4900e+02],\n",
      "        [1.6500e+02, 6.1900e+02, 2.1000e+02, 6.6300e+02],\n",
      "        [1.1900e+02, 6.3200e+02, 1.6600e+02, 6.7700e+02],\n",
      "        [7.3000e+01, 6.4900e+02, 1.2000e+02, 6.8800e+02],\n",
      "        [1.9000e+01, 6.6400e+02, 6.7000e+01, 7.0300e+02],\n",
      "        [1.3900e+02, 6.6600e+02, 1.9300e+02, 7.2000e+02],\n",
      "        [4.6400e+02, 4.1500e+02, 5.0400e+02, 4.4700e+02],\n",
      "        [5.2500e+02, 3.8800e+02, 5.6600e+02, 4.1300e+02],\n",
      "        [4.5500e+02, 3.7900e+02, 4.8700e+02, 4.0200e+02],\n",
      "        [4.8300e+02, 3.6600e+02, 5.2300e+02, 3.8500e+02],\n",
      "        [5.2300e+02, 3.4700e+02, 5.6200e+02, 3.6400e+02],\n",
      "        [5.6900e+02, 3.5900e+02, 6.0900e+02, 3.7700e+02],\n",
      "        [5.9300e+02, 3.5100e+02, 6.2600e+02, 3.6800e+02],\n",
      "        [5.7700e+02, 3.9200e+02, 6.1700e+02, 4.1800e+02],\n",
      "        [6.2000e+02, 3.9000e+02, 6.5600e+02, 4.1000e+02],\n",
      "        [6.2700e+02, 3.7000e+02, 6.6300e+02, 3.9200e+02],\n",
      "        [6.4800e+02, 3.6600e+02, 6.8000e+02, 3.8700e+02],\n",
      "        [6.5000e+02, 3.3100e+02, 6.8800e+02, 3.4900e+02],\n",
      "        [6.9500e+02, 3.6800e+02, 7.3100e+02, 3.9200e+02],\n",
      "        [6.7300e+02, 4.0800e+02, 7.1400e+02, 4.2900e+02],\n",
      "        [7.6100e+02, 3.5900e+02, 7.8700e+02, 3.7900e+02],\n",
      "        [7.2900e+02, 3.5000e+02, 7.6500e+02, 3.7100e+02],\n",
      "        [7.7700e+02, 4.4700e+02, 8.2900e+02, 4.7300e+02],\n",
      "        [8.8800e+02, 4.3500e+02, 9.4200e+02, 4.6800e+02],\n",
      "        [6.6000e+02, 4.7700e+02, 7.1200e+02, 5.0600e+02],\n",
      "        [6.0900e+02, 4.2400e+02, 6.5100e+02, 4.4500e+02],\n",
      "        [5.1300e+02, 4.4400e+02, 5.5300e+02, 4.7200e+02],\n",
      "        [5.4200e+02, 4.5500e+02, 5.9200e+02, 4.9100e+02],\n",
      "        [6.8200e+02, 5.0300e+02, 7.3700e+02, 5.3900e+02],\n",
      "        [7.2700e+02, 3.9000e+02, 7.7100e+02, 4.1600e+02],\n",
      "        [7.5500e+02, 3.8700e+02, 7.8700e+02, 4.1200e+02],\n",
      "        [8.2300e+02, 3.7600e+02, 8.6600e+02, 3.9800e+02],\n",
      "        [8.5800e+02, 3.8100e+02, 8.9800e+02, 4.0700e+02],\n",
      "        [8.7200e+02, 3.8800e+02, 9.1300e+02, 4.1400e+02],\n",
      "        [8.9400e+02, 3.9500e+02, 9.3500e+02, 4.1800e+02],\n",
      "        [9.1900e+02, 4.0200e+02, 9.6000e+02, 4.2500e+02],\n",
      "        [9.4000e+02, 4.0800e+02, 9.8500e+02, 4.3000e+02],\n",
      "        [7.3900e+02, 5.4800e+02, 8.0500e+02, 5.9300e+02],\n",
      "        [6.4900e+02, 5.8500e+02, 7.1900e+02, 6.2900e+02],\n",
      "        [6.6700e+02, 6.2500e+02, 7.4300e+02, 6.8400e+02],\n",
      "        [6.0900e+02, 5.9800e+02, 6.8000e+02, 6.4900e+02],\n",
      "        [5.9300e+02, 6.5800e+02, 6.6500e+02, 7.1400e+02],\n",
      "        [4.6200e+02, 6.4700e+02, 5.2700e+02, 7.0300e+02],\n",
      "        [5.1200e+02, 6.8700e+02, 5.7800e+02, 7.2000e+02],\n",
      "        [8.6001e+01, 6.7800e+02, 1.4200e+02, 7.2000e+02],\n",
      "        [1.0760e+03, 4.3800e+02, 1.1300e+03, 4.7400e+02],\n",
      "        [1.2040e+03, 4.8200e+02, 1.2790e+03, 5.2400e+02]])\n",
      "Labels: tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 4, 4, 4])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, -1, 91, 96, 168]' is invalid for input of size 64512",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBoxes: \u001b[39m\u001b[39m{\u001b[39;00mtarget[\u001b[39m'\u001b[39m\u001b[39mboxes\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLabels: \u001b[39m\u001b[39m{\u001b[39;00mtarget[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[0;32m     18\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\retinanet.py:633\u001b[0m, in \u001b[0;36mRetinaNet.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    630\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(features\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m    632\u001b[0m \u001b[39m# compute the retinanet heads outputs using the features\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m head_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead(features)\n\u001b[0;32m    635\u001b[0m \u001b[39m# create the set of anchors\u001b[39;00m\n\u001b[0;32m    636\u001b[0m anchors \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39manchor_generator(images, features)\n",
      "File \u001b[1;32mc:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\retinanet.py:84\u001b[0m, in \u001b[0;36mRetinaNetHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     83\u001b[0m     \u001b[39m# type: (List[Tensor]) -> Dict[str, Tensor]\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mcls_logits\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassification_head(x), \u001b[39m\"\u001b[39m\u001b[39mbbox_regression\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregression_head(x)}\n",
      "File \u001b[1;32mc:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\detection\\retinanet.py:201\u001b[0m, in \u001b[0;36mRetinaNetClassificationHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m# Permute classification output from (N, A * K, H, W) to (N, HWA, K).\u001b[39;00m\n\u001b[0;32m    200\u001b[0m N, _, H, W \u001b[39m=\u001b[39m cls_logits\u001b[39m.\u001b[39mshape\n\u001b[1;32m--> 201\u001b[0m cls_logits \u001b[39m=\u001b[39m cls_logits\u001b[39m.\u001b[39;49mview(N, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_classes, H, W)\n\u001b[0;32m    202\u001b[0m cls_logits \u001b[39m=\u001b[39m cls_logits\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m    203\u001b[0m cls_logits \u001b[39m=\u001b[39m cls_logits\u001b[39m.\u001b[39mreshape(N, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)  \u001b[39m# Size=(N, HWA, 4)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, -1, 91, 96, 168]' is invalid for input of size 64512"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, targets) in enumerate(train_dataloader):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Debug: print out the targets to check their values\n",
    "        if i == 0 and epoch == 0:\n",
    "            print(\"Debug output:\")\n",
    "            for target in targets:\n",
    "                print(f\"Boxes: {target['boxes']}\")\n",
    "                print(f\"Labels: {target['labels']}\")\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {losses.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
