{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download labeling from GitHub - https://github.com/tzutalin/labelImg\n",
    "\n",
    "\n",
    "`!pip install pyqt5`\n",
    "\n",
    "`!pip install lxml`\n",
    "\n",
    "\n",
    "Installation guide - https://github.com/heartexlabs/labelImg#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command in the terminal\n",
    "\n",
    "`pyrcc5 -o libs/resources.py resources.qrc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=RetinaNet_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.retinanet import RetinaNet_ResNet50_FPN_Weights\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Number of classes including background\n",
    "num_classes = 91 \n",
    "\n",
    "# Load a pre-trained model\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True, num_classes=num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Continue with your code...(Origional numbers)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform if transform else transforms.ToTensor()\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"images\"))))\n",
    "        self.labels = list(sorted(os.listdir(os.path.join(root_dir, \"labels\"))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, \"images\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.root_dir, \"labels\", self.labels[idx])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "        # Apply transformation after getting original size\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # Read YOLO label file\n",
    "        with open(label_path, \"r\") as file:\n",
    "            lines = file.read().splitlines()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "            labels.append(int(class_id))\n",
    "\n",
    "            x_min = img_width * (x_center - width / 2)\n",
    "            y_min = img_height * (y_center - height / 2)\n",
    "            x_max = img_width * (x_center + width / 2)\n",
    "            y_max = img_height * (y_center + height / 2)\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.tensor(boxes, dtype=torch.float32)\n",
    "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# Define your own paths here\n",
    "train_dataset = YOLODataset(\"data/train set\")\n",
    "valid_dataset = YOLODataset(\"data/validation set\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.0734055042266846\n",
      "Epoch: 2, Loss: 0.528253436088562\n",
      "Epoch: 3, Loss: 0.36961835622787476\n",
      "Epoch: 4, Loss: 0.35107991099357605\n",
      "Epoch: 5, Loss: 0.3144419193267822\n",
      "Epoch: 6, Loss: 0.28877097368240356\n",
      "Epoch: 7, Loss: 0.31749796867370605\n",
      "Epoch: 8, Loss: 0.283780574798584\n",
      "Epoch: 9, Loss: 0.324700266122818\n",
      "Epoch: 10, Loss: 0.2940836250782013\n",
      "Epoch: 11, Loss: 0.25590741634368896\n",
      "Epoch: 12, Loss: 0.24234220385551453\n",
      "Epoch: 13, Loss: 0.2431105673313141\n",
      "Epoch: 14, Loss: 0.23800525069236755\n",
      "Epoch: 15, Loss: 0.2319270670413971\n",
      "Epoch: 16, Loss: 0.21747547388076782\n",
      "Epoch: 17, Loss: 0.26543816924095154\n",
      "Epoch: 18, Loss: 0.2664937376976013\n",
      "Epoch: 19, Loss: 0.21988548338413239\n",
      "Epoch: 20, Loss: 0.21528971195220947\n",
      "Epoch: 21, Loss: 0.20219314098358154\n",
      "Epoch: 22, Loss: 0.21074198186397552\n",
      "Epoch: 23, Loss: 0.24529071152210236\n",
      "Epoch: 24, Loss: 0.2017199695110321\n",
      "Epoch: 25, Loss: 0.23313021659851074\n",
      "Epoch: 26, Loss: 0.1993691623210907\n",
      "Epoch: 27, Loss: 0.17716029286384583\n",
      "Epoch: 28, Loss: 0.17390038073062897\n",
      "Epoch: 29, Loss: 0.17067033052444458\n",
      "Epoch: 30, Loss: 0.17716297507286072\n",
      "Epoch: 31, Loss: 0.2106989622116089\n",
      "Epoch: 32, Loss: 0.17741365730762482\n",
      "Epoch: 33, Loss: 0.21699769794940948\n",
      "Epoch: 34, Loss: 0.1854275017976761\n",
      "Epoch: 35, Loss: 0.18319541215896606\n",
      "Epoch: 36, Loss: 0.18202154338359833\n",
      "Epoch: 37, Loss: 0.19775331020355225\n",
      "Epoch: 38, Loss: 0.2076294869184494\n",
      "Epoch: 39, Loss: 0.17103470861911774\n",
      "Epoch: 40, Loss: 0.1887437403202057\n",
      "Epoch: 41, Loss: 0.1751110851764679\n",
      "Epoch: 42, Loss: 0.1806935966014862\n",
      "Epoch: 43, Loss: 0.1381554752588272\n",
      "Epoch: 44, Loss: 0.16034278273582458\n",
      "Epoch: 45, Loss: 0.15929722785949707\n",
      "Epoch: 46, Loss: 0.1711498647928238\n",
      "Epoch: 47, Loss: 0.18703775107860565\n",
      "Epoch: 48, Loss: 0.13266968727111816\n",
      "Epoch: 49, Loss: 0.15447749197483063\n",
      "Epoch: 50, Loss: 0.1292296200990677\n",
      "Epoch: 51, Loss: 0.2248975932598114\n",
      "Epoch: 52, Loss: 0.1599310040473938\n",
      "Epoch: 53, Loss: 0.1641702950000763\n",
      "Epoch: 54, Loss: 0.1574079543352127\n",
      "Epoch: 55, Loss: 0.16408440470695496\n",
      "Epoch: 56, Loss: 0.17846110463142395\n",
      "Epoch: 57, Loss: 0.1773746758699417\n",
      "Epoch: 58, Loss: 0.16043299436569214\n",
      "Epoch: 59, Loss: 0.1542503982782364\n",
      "Epoch: 60, Loss: 0.17292079329490662\n",
      "Epoch: 61, Loss: 0.14837390184402466\n",
      "Epoch: 62, Loss: 0.147085502743721\n",
      "Epoch: 63, Loss: 0.18174022436141968\n",
      "Epoch: 64, Loss: 0.14459991455078125\n",
      "Epoch: 65, Loss: 0.14466337859630585\n",
      "Epoch: 66, Loss: 0.14974316954612732\n",
      "Epoch: 67, Loss: 0.14658692479133606\n",
      "Epoch: 68, Loss: 0.1420302838087082\n",
      "Epoch: 69, Loss: 0.20857378840446472\n",
      "Epoch: 70, Loss: 0.1094043105840683\n",
      "Epoch: 71, Loss: 0.14732199907302856\n",
      "Epoch: 72, Loss: 0.20210634171962738\n",
      "Epoch: 73, Loss: 0.1846117079257965\n",
      "Epoch: 74, Loss: 0.13874699175357819\n",
      "Epoch: 75, Loss: 0.1170186996459961\n",
      "Epoch: 76, Loss: 0.14500391483306885\n",
      "Epoch: 77, Loss: 0.1368536800146103\n",
      "Epoch: 78, Loss: 0.14731082320213318\n",
      "Epoch: 79, Loss: 0.1123000830411911\n",
      "Epoch: 80, Loss: 0.14593341946601868\n",
      "Epoch: 81, Loss: 0.17964236438274384\n",
      "Epoch: 82, Loss: 0.11090660095214844\n",
      "Epoch: 83, Loss: 0.13360261917114258\n",
      "Epoch: 84, Loss: 0.1436496078968048\n",
      "Epoch: 85, Loss: 0.13147900998592377\n",
      "Epoch: 86, Loss: 0.1527714729309082\n",
      "Epoch: 87, Loss: 0.17620903253555298\n",
      "Epoch: 88, Loss: 0.1293516308069229\n",
      "Epoch: 89, Loss: 0.12691554427146912\n",
      "Epoch: 90, Loss: 0.13051950931549072\n",
      "Epoch: 91, Loss: 0.13903522491455078\n",
      "Epoch: 92, Loss: 0.1379248946905136\n",
      "Epoch: 93, Loss: 0.1296582967042923\n",
      "Epoch: 94, Loss: 0.0951273962855339\n",
      "Epoch: 95, Loss: 0.10352374613285065\n",
      "Epoch: 96, Loss: 0.18601185083389282\n",
      "Epoch: 97, Loss: 0.14838296175003052\n",
      "Epoch: 98, Loss: 0.14062979817390442\n",
      "Epoch: 99, Loss: 0.1483478844165802\n",
      "Epoch: 100, Loss: 0.1691049486398697\n",
      "Epoch: 101, Loss: 0.18120388686656952\n",
      "Epoch: 102, Loss: 0.1676812767982483\n",
      "Epoch: 103, Loss: 0.13628923892974854\n",
      "Epoch: 104, Loss: 0.0906965583562851\n",
      "Epoch: 105, Loss: 0.1660367250442505\n",
      "Epoch: 106, Loss: 0.13693109154701233\n",
      "Epoch: 107, Loss: 0.1647622287273407\n",
      "Epoch: 108, Loss: 0.1279028058052063\n",
      "Epoch: 109, Loss: 0.1633523851633072\n",
      "Epoch: 110, Loss: 0.12352143228054047\n",
      "Epoch: 111, Loss: 0.14216890931129456\n",
      "Epoch: 112, Loss: 0.13493748009204865\n",
      "Epoch: 113, Loss: 0.13240328431129456\n",
      "Epoch: 114, Loss: 0.09651276469230652\n",
      "Epoch: 115, Loss: 0.13088452816009521\n",
      "Epoch: 116, Loss: 0.1258355975151062\n",
      "Epoch: 117, Loss: 0.12151416391134262\n",
      "Epoch: 118, Loss: 0.08506261557340622\n",
      "Epoch: 119, Loss: 0.12010219693183899\n",
      "Epoch: 120, Loss: 0.17007847130298615\n",
      "Epoch: 121, Loss: 0.13731466233730316\n",
      "Epoch: 122, Loss: 0.12945492565631866\n",
      "Epoch: 123, Loss: 0.09369676560163498\n",
      "Epoch: 124, Loss: 0.1394488364458084\n",
      "Epoch: 125, Loss: 0.13868892192840576\n",
      "Epoch: 126, Loss: 0.14899757504463196\n",
      "Epoch: 127, Loss: 0.1083640605211258\n",
      "Epoch: 128, Loss: 0.1564529687166214\n",
      "Epoch: 129, Loss: 0.13018250465393066\n",
      "Epoch: 130, Loss: 0.14690998196601868\n",
      "Epoch: 131, Loss: 0.16475419700145721\n",
      "Epoch: 132, Loss: 0.0908542051911354\n",
      "Epoch: 133, Loss: 0.09043751657009125\n",
      "Epoch: 134, Loss: 0.1450011432170868\n",
      "Epoch: 135, Loss: 0.1618758887052536\n",
      "Epoch: 136, Loss: 0.1599002182483673\n",
      "Epoch: 137, Loss: 0.08099516481161118\n",
      "Epoch: 138, Loss: 0.1305159032344818\n",
      "Epoch: 139, Loss: 0.11488800495862961\n",
      "Epoch: 140, Loss: 0.1343163251876831\n",
      "Epoch: 141, Loss: 0.13372893631458282\n",
      "Epoch: 142, Loss: 0.14217329025268555\n",
      "Epoch: 143, Loss: 0.10211935639381409\n",
      "Epoch: 144, Loss: 0.1284875124692917\n",
      "Epoch: 145, Loss: 0.15724913775920868\n",
      "Epoch: 146, Loss: 0.1232915073633194\n",
      "Epoch: 147, Loss: 0.14980293810367584\n",
      "Epoch: 148, Loss: 0.11279550194740295\n",
      "Epoch: 149, Loss: 0.08648928254842758\n",
      "Epoch: 150, Loss: 0.13128019869327545\n",
      "Epoch: 151, Loss: 0.11183136701583862\n",
      "Epoch: 152, Loss: 0.13926133513450623\n",
      "Epoch: 153, Loss: 0.13839052617549896\n",
      "Epoch: 154, Loss: 0.1303027719259262\n",
      "Epoch: 155, Loss: 0.11065036058425903\n",
      "Epoch: 156, Loss: 0.08437022566795349\n",
      "Epoch: 157, Loss: 0.096196748316288\n",
      "Epoch: 158, Loss: 0.12009888887405396\n",
      "Epoch: 159, Loss: 0.11648518592119217\n",
      "Epoch: 160, Loss: 0.12319344282150269\n",
      "Epoch: 161, Loss: 0.12033823132514954\n",
      "Epoch: 162, Loss: 0.14539307355880737\n",
      "Epoch: 163, Loss: 0.12010478228330612\n",
      "Epoch: 164, Loss: 0.14797191321849823\n",
      "Epoch: 165, Loss: 0.1343492865562439\n",
      "Epoch: 166, Loss: 0.1179204136133194\n",
      "Epoch: 167, Loss: 0.1211564689874649\n",
      "Epoch: 168, Loss: 0.14327672123908997\n",
      "Epoch: 169, Loss: 0.14292702078819275\n",
      "Epoch: 170, Loss: 0.13321344554424286\n",
      "Epoch: 171, Loss: 0.1268702894449234\n",
      "Epoch: 172, Loss: 0.1198669821023941\n",
      "Epoch: 173, Loss: 0.14130175113677979\n",
      "Epoch: 174, Loss: 0.12572868168354034\n",
      "Epoch: 175, Loss: 0.11339469999074936\n",
      "Epoch: 176, Loss: 0.08057339489459991\n",
      "Epoch: 177, Loss: 0.07265665382146835\n",
      "Epoch: 178, Loss: 0.08946138620376587\n",
      "Epoch: 179, Loss: 0.08830412477254868\n",
      "Epoch: 180, Loss: 0.08685764670372009\n",
      "Epoch: 181, Loss: 0.0858779028058052\n",
      "Epoch: 182, Loss: 0.1300479769706726\n",
      "Epoch: 183, Loss: 0.07128553092479706\n",
      "Epoch: 184, Loss: 0.07853338867425919\n",
      "Epoch: 185, Loss: 0.07104818522930145\n",
      "Epoch: 186, Loss: 0.14074347913265228\n",
      "Epoch: 187, Loss: 0.0853910818696022\n",
      "Epoch: 188, Loss: 0.11585090309381485\n",
      "Epoch: 189, Loss: 0.07798484712839127\n",
      "Epoch: 190, Loss: 0.11657149344682693\n",
      "Epoch: 191, Loss: 0.1390589475631714\n",
      "Epoch: 192, Loss: 0.07667107880115509\n",
      "Epoch: 193, Loss: 0.13698484003543854\n",
      "Epoch: 194, Loss: 0.13592280447483063\n",
      "Epoch: 195, Loss: 0.07025452703237534\n",
      "Epoch: 196, Loss: 0.11007224023342133\n",
      "Epoch: 197, Loss: 0.11423154175281525\n",
      "Epoch: 198, Loss: 0.10358179360628128\n",
      "Epoch: 199, Loss: 0.12573572993278503\n",
      "Epoch: 200, Loss: 0.13432735204696655\n",
      "Epoch: 201, Loss: 0.1147613674402237\n",
      "Epoch: 202, Loss: 0.1340366005897522\n",
      "Epoch: 203, Loss: 0.10221600532531738\n",
      "Epoch: 204, Loss: 0.12440518289804459\n",
      "Epoch: 205, Loss: 0.07481838762760162\n",
      "Epoch: 206, Loss: 0.07433324307203293\n",
      "Epoch: 207, Loss: 0.13304805755615234\n",
      "Epoch: 208, Loss: 0.13204334676265717\n",
      "Epoch: 209, Loss: 0.10936109721660614\n",
      "Epoch: 210, Loss: 0.08025834709405899\n",
      "Epoch: 211, Loss: 0.11094195395708084\n",
      "Epoch: 212, Loss: 0.11059672385454178\n",
      "Epoch: 213, Loss: 0.07270126789808273\n",
      "Epoch: 214, Loss: 0.13126714527606964\n",
      "Epoch: 215, Loss: 0.11260601133108139\n",
      "Epoch: 216, Loss: 0.10784342885017395\n",
      "Epoch: 217, Loss: 0.10053852945566177\n",
      "Epoch: 218, Loss: 0.10721855610609055\n",
      "Epoch: 219, Loss: 0.12119508534669876\n",
      "Epoch: 220, Loss: 0.07138712704181671\n",
      "Epoch: 221, Loss: 0.09958295524120331\n",
      "Epoch: 222, Loss: 0.11733227968215942\n",
      "Epoch: 223, Loss: 0.0701221451163292\n",
      "Epoch: 224, Loss: 0.0989895835518837\n",
      "Epoch: 225, Loss: 0.11968916654586792\n",
      "Epoch: 226, Loss: 0.10749652981758118\n",
      "Epoch: 227, Loss: 0.12824667990207672\n",
      "Epoch: 228, Loss: 0.09801629185676575\n",
      "Epoch: 229, Loss: 0.06663866341114044\n",
      "Epoch: 230, Loss: 0.12532539665699005\n",
      "Epoch: 231, Loss: 0.09749379754066467\n",
      "Epoch: 232, Loss: 0.0697304904460907\n",
      "Epoch: 233, Loss: 0.1049918532371521\n",
      "Epoch: 234, Loss: 0.1177469789981842\n",
      "Epoch: 235, Loss: 0.11550760269165039\n",
      "Epoch: 236, Loss: 0.10894127190113068\n",
      "Epoch: 237, Loss: 0.10830853879451752\n",
      "Epoch: 238, Loss: 0.11496078968048096\n",
      "Epoch: 239, Loss: 0.07344966381788254\n",
      "Epoch: 240, Loss: 0.12274404615163803\n",
      "Epoch: 241, Loss: 0.064682736992836\n",
      "Epoch: 242, Loss: 0.10325002670288086\n",
      "Epoch: 243, Loss: 0.1160636693239212\n",
      "Epoch: 244, Loss: 0.10825322568416595\n",
      "Epoch: 245, Loss: 0.10697478801012039\n",
      "Epoch: 246, Loss: 0.10374578088521957\n",
      "Epoch: 247, Loss: 0.1133604496717453\n",
      "Epoch: 248, Loss: 0.10692102462053299\n",
      "Epoch: 249, Loss: 0.06759458780288696\n",
      "Epoch: 250, Loss: 0.10589330643415451\n",
      "Epoch: 251, Loss: 0.11256402730941772\n",
      "Epoch: 252, Loss: 0.11433234810829163\n",
      "Epoch: 253, Loss: 0.10137545317411423\n",
      "Epoch: 254, Loss: 0.07055864483118057\n",
      "Epoch: 255, Loss: 0.11854114383459091\n",
      "Epoch: 256, Loss: 0.10184277594089508\n",
      "Epoch: 257, Loss: 0.06639671325683594\n",
      "Epoch: 258, Loss: 0.06569959968328476\n",
      "Epoch: 259, Loss: 0.11138398200273514\n",
      "Epoch: 260, Loss: 0.11269191652536392\n",
      "Epoch: 261, Loss: 0.10080380737781525\n",
      "Epoch: 262, Loss: 0.09404018521308899\n",
      "Epoch: 263, Loss: 0.09373979270458221\n",
      "Epoch: 264, Loss: 0.10611461848020554\n",
      "Epoch: 265, Loss: 0.09991692006587982\n",
      "Epoch: 266, Loss: 0.10527590662240982\n",
      "Epoch: 267, Loss: 0.11119985580444336\n",
      "Epoch: 268, Loss: 0.1040520966053009\n",
      "Epoch: 269, Loss: 0.10432315617799759\n",
      "Epoch: 270, Loss: 0.10068502277135849\n",
      "Epoch: 271, Loss: 0.10366678237915039\n",
      "Epoch: 272, Loss: 0.09279265999794006\n",
      "Epoch: 273, Loss: 0.061436232179403305\n",
      "Epoch: 274, Loss: 0.11959962546825409\n",
      "Epoch: 275, Loss: 0.06379018723964691\n",
      "Epoch: 276, Loss: 0.06090202182531357\n",
      "Epoch: 277, Loss: 0.10918682813644409\n",
      "Epoch: 278, Loss: 0.10359276086091995\n",
      "Epoch: 279, Loss: 0.1030469536781311\n",
      "Epoch: 280, Loss: 0.06575609743595123\n",
      "Epoch: 281, Loss: 0.10893494635820389\n",
      "Epoch: 282, Loss: 0.10862299799919128\n",
      "Epoch: 283, Loss: 0.09154526889324188\n",
      "Epoch: 284, Loss: 0.09884777665138245\n",
      "Epoch: 285, Loss: 0.10234439373016357\n",
      "Epoch: 286, Loss: 0.11160457134246826\n",
      "Epoch: 287, Loss: 0.0962459146976471\n",
      "Epoch: 288, Loss: 0.11706746369600296\n",
      "Epoch: 289, Loss: 0.10182710736989975\n",
      "Epoch: 290, Loss: 0.06435927748680115\n",
      "Epoch: 291, Loss: 0.1074129194021225\n",
      "Epoch: 292, Loss: 0.1162390261888504\n",
      "Epoch: 293, Loss: 0.06334669142961502\n",
      "Epoch: 294, Loss: 0.10125337541103363\n",
      "Epoch: 295, Loss: 0.10101445019245148\n",
      "Epoch: 296, Loss: 0.10640246421098709\n",
      "Epoch: 297, Loss: 0.06123994290828705\n",
      "Epoch: 298, Loss: 0.10914355516433716\n",
      "Epoch: 299, Loss: 0.10657477378845215\n",
      "Epoch: 300, Loss: 0.06237686052918434\n",
      "Epoch: 301, Loss: 0.059052687138319016\n",
      "Epoch: 302, Loss: 0.10536125302314758\n",
      "Epoch: 303, Loss: 0.1051422655582428\n",
      "Epoch: 304, Loss: 0.10016785562038422\n",
      "Epoch: 305, Loss: 0.10431045293807983\n",
      "Epoch: 306, Loss: 0.10733410716056824\n",
      "Epoch: 307, Loss: 0.10535501688718796\n",
      "Epoch: 308, Loss: 0.09303510189056396\n",
      "Epoch: 309, Loss: 0.05785030871629715\n",
      "Epoch: 310, Loss: 0.10355470329523087\n",
      "Epoch: 311, Loss: 0.09244388341903687\n",
      "Epoch: 312, Loss: 0.09221641719341278\n",
      "Epoch: 313, Loss: 0.05989723652601242\n",
      "Epoch: 314, Loss: 0.05712087079882622\n",
      "Epoch: 315, Loss: 0.05696678161621094\n",
      "Epoch: 316, Loss: 0.10532040148973465\n",
      "Epoch: 317, Loss: 0.10502573102712631\n",
      "Epoch: 318, Loss: 0.06002603471279144\n",
      "Epoch: 319, Loss: 0.09503764659166336\n",
      "Epoch: 320, Loss: 0.09859375655651093\n",
      "Epoch: 321, Loss: 0.11212383210659027\n",
      "Epoch: 322, Loss: 0.10370545089244843\n",
      "Epoch: 323, Loss: 0.056166909635066986\n",
      "Epoch: 324, Loss: 0.10349804162979126\n",
      "Epoch: 325, Loss: 0.10302815586328506\n",
      "Epoch: 326, Loss: 0.058552540838718414\n",
      "Epoch: 327, Loss: 0.09189166128635406\n",
      "Epoch: 328, Loss: 0.05528602376580238\n",
      "Epoch: 329, Loss: 0.057750873267650604\n",
      "Epoch: 330, Loss: 0.05491495132446289\n",
      "Epoch: 331, Loss: 0.09108251333236694\n",
      "Epoch: 332, Loss: 0.11048810184001923\n",
      "Epoch: 333, Loss: 0.0974738597869873\n",
      "Epoch: 334, Loss: 0.09710666537284851\n",
      "Epoch: 335, Loss: 0.08909261226654053\n",
      "Epoch: 336, Loss: 0.10160495340824127\n",
      "Epoch: 337, Loss: 0.10973577946424484\n",
      "Epoch: 338, Loss: 0.1091652512550354\n",
      "Epoch: 339, Loss: 0.0967244952917099\n",
      "Epoch: 340, Loss: 0.09987379610538483\n",
      "Epoch: 341, Loss: 0.09969782829284668\n",
      "Epoch: 342, Loss: 0.09616715461015701\n",
      "Epoch: 343, Loss: 0.09891198575496674\n",
      "Epoch: 344, Loss: 0.05412984639406204\n",
      "Epoch: 345, Loss: 0.09563838690519333\n",
      "Epoch: 346, Loss: 0.099697545170784\n",
      "Epoch: 347, Loss: 0.05717507004737854\n",
      "Epoch: 348, Loss: 0.10094640403985977\n",
      "Epoch: 349, Loss: 0.08910572528839111\n",
      "Epoch: 350, Loss: 0.055434755980968475\n",
      "Epoch: 351, Loss: 0.08528794348239899\n",
      "Epoch: 352, Loss: 0.10742024332284927\n",
      "Epoch: 353, Loss: 0.08478978276252747\n",
      "Epoch: 354, Loss: 0.05357852578163147\n",
      "Epoch: 355, Loss: 0.05459607020020485\n",
      "Epoch: 356, Loss: 0.09489348530769348\n",
      "Epoch: 357, Loss: 0.0862753614783287\n",
      "Epoch: 358, Loss: 0.05580201372504234\n",
      "Epoch: 359, Loss: 0.09774376451969147\n",
      "Epoch: 360, Loss: 0.0973600447177887\n",
      "Epoch: 361, Loss: 0.05535043030977249\n",
      "Epoch: 362, Loss: 0.05381981283426285\n",
      "Epoch: 363, Loss: 0.08740507066249847\n",
      "Epoch: 364, Loss: 0.08529067784547806\n",
      "Epoch: 365, Loss: 0.08504726737737656\n",
      "Epoch: 366, Loss: 0.10564421862363815\n",
      "Epoch: 367, Loss: 0.08468185365200043\n",
      "Epoch: 368, Loss: 0.054416753351688385\n",
      "Epoch: 369, Loss: 0.05282565951347351\n",
      "Epoch: 370, Loss: 0.08654514700174332\n",
      "Epoch: 371, Loss: 0.09589392691850662\n",
      "Epoch: 372, Loss: 0.09547693282365799\n",
      "Epoch: 373, Loss: 0.052197061479091644\n",
      "Epoch: 374, Loss: 0.09820321947336197\n",
      "Epoch: 375, Loss: 0.09487922489643097\n",
      "Epoch: 376, Loss: 0.09452048689126968\n",
      "Epoch: 377, Loss: 0.051648736000061035\n",
      "Epoch: 378, Loss: 0.08937785774469376\n",
      "Epoch: 379, Loss: 0.09750455617904663\n",
      "Epoch: 380, Loss: 0.10378450900316238\n",
      "Epoch: 381, Loss: 0.1036001667380333\n",
      "Epoch: 382, Loss: 0.08870252221822739\n",
      "Epoch: 383, Loss: 0.09452828019857407\n",
      "Epoch: 384, Loss: 0.08274735510349274\n",
      "Epoch: 385, Loss: 0.05310003459453583\n",
      "Epoch: 386, Loss: 0.08201999217271805\n",
      "Epoch: 387, Loss: 0.09371798485517502\n",
      "Epoch: 388, Loss: 0.08819926530122757\n",
      "Epoch: 389, Loss: 0.10259092599153519\n",
      "Epoch: 390, Loss: 0.08200004696846008\n",
      "Epoch: 391, Loss: 0.10225177556276321\n",
      "Epoch: 392, Loss: 0.09215113520622253\n",
      "Epoch: 393, Loss: 0.08123866468667984\n",
      "Epoch: 394, Loss: 0.05211326852440834\n",
      "Epoch: 395, Loss: 0.09191899746656418\n",
      "Epoch: 396, Loss: 0.04967767000198364\n",
      "Epoch: 397, Loss: 0.0958840399980545\n",
      "Epoch: 398, Loss: 0.05160121992230415\n",
      "Epoch: 399, Loss: 0.08727115392684937\n",
      "Epoch: 400, Loss: 0.09152830392122269\n",
      "Epoch: 401, Loss: 0.08347317576408386\n",
      "Epoch: 402, Loss: 0.09525440633296967\n",
      "Epoch: 403, Loss: 0.08666741102933884\n",
      "Epoch: 404, Loss: 0.0829927921295166\n",
      "Epoch: 405, Loss: 0.08275315165519714\n",
      "Epoch: 406, Loss: 0.10050913691520691\n",
      "Epoch: 407, Loss: 0.08005891740322113\n",
      "Epoch: 408, Loss: 0.05074102431535721\n",
      "Epoch: 409, Loss: 0.09987159073352814\n",
      "Epoch: 410, Loss: 0.0945538729429245\n",
      "Epoch: 411, Loss: 0.09944517910480499\n",
      "Epoch: 412, Loss: 0.0795673131942749\n",
      "Epoch: 413, Loss: 0.04767460376024246\n",
      "Epoch: 414, Loss: 0.05038432404398918\n",
      "Epoch: 415, Loss: 0.0503874197602272\n",
      "Epoch: 416, Loss: 0.09909510612487793\n",
      "Epoch: 417, Loss: 0.07899574190378189\n",
      "Epoch: 418, Loss: 0.09039615839719772\n",
      "Epoch: 419, Loss: 0.09857138991355896\n",
      "Epoch: 420, Loss: 0.08985748142004013\n",
      "Epoch: 421, Loss: 0.07872036844491959\n",
      "Epoch: 422, Loss: 0.0787273570895195\n",
      "Epoch: 423, Loss: 0.049726590514183044\n",
      "Epoch: 424, Loss: 0.07843822985887527\n",
      "Epoch: 425, Loss: 0.08918534219264984\n",
      "Epoch: 426, Loss: 0.07828297466039658\n",
      "Epoch: 427, Loss: 0.07807119935750961\n",
      "Epoch: 428, Loss: 0.07790625095367432\n",
      "Epoch: 429, Loss: 0.08891233801841736\n",
      "Epoch: 430, Loss: 0.09315398335456848\n",
      "Epoch: 431, Loss: 0.08853577822446823\n",
      "Epoch: 432, Loss: 0.07766199856996536\n",
      "Epoch: 433, Loss: 0.07735678553581238\n",
      "Epoch: 434, Loss: 0.08020345866680145\n",
      "Epoch: 435, Loss: 0.04584590345621109\n",
      "Epoch: 436, Loss: 0.04880529269576073\n",
      "Epoch: 437, Loss: 0.07716988027095795\n",
      "Epoch: 438, Loss: 0.0485340841114521\n",
      "Epoch: 439, Loss: 0.07944207638502121\n",
      "Epoch: 440, Loss: 0.08667978644371033\n",
      "Epoch: 441, Loss: 0.07917124032974243\n",
      "Epoch: 442, Loss: 0.047664105892181396\n",
      "Epoch: 443, Loss: 0.09194209426641464\n",
      "Epoch: 444, Loss: 0.04456684738397598\n",
      "Epoch: 445, Loss: 0.08600784838199615\n",
      "Epoch: 446, Loss: 0.07645849883556366\n",
      "Epoch: 447, Loss: 0.0875791534781456\n",
      "Epoch: 448, Loss: 0.04701919108629227\n",
      "Epoch: 449, Loss: 0.083372101187706\n",
      "Epoch: 450, Loss: 0.07590021193027496\n",
      "Epoch: 451, Loss: 0.08691096305847168\n",
      "Epoch: 452, Loss: 0.08500920981168747\n",
      "Epoch: 453, Loss: 0.08303267508745193\n",
      "Epoch: 454, Loss: 0.07818957418203354\n",
      "Epoch: 455, Loss: 0.0864069014787674\n",
      "Epoch: 456, Loss: 0.07521246373653412\n",
      "Epoch: 457, Loss: 0.08735907822847366\n",
      "Epoch: 458, Loss: 0.08413468301296234\n",
      "Epoch: 459, Loss: 0.07559343427419662\n",
      "Epoch: 460, Loss: 0.0776050016283989\n",
      "Epoch: 461, Loss: 0.04317690432071686\n",
      "Epoch: 462, Loss: 0.08371029794216156\n",
      "Epoch: 463, Loss: 0.04613140970468521\n",
      "Epoch: 464, Loss: 0.09376023709774017\n",
      "Epoch: 465, Loss: 0.08534123748540878\n",
      "Epoch: 466, Loss: 0.09351521730422974\n",
      "Epoch: 467, Loss: 0.07421433180570602\n",
      "Epoch: 468, Loss: 0.07404056191444397\n",
      "Epoch: 469, Loss: 0.08574096858501434\n",
      "Epoch: 470, Loss: 0.04238228499889374\n",
      "Epoch: 471, Loss: 0.07365156710147858\n",
      "Epoch: 472, Loss: 0.08974550664424896\n",
      "Epoch: 473, Loss: 0.04177560284733772\n",
      "Epoch: 474, Loss: 0.07328609377145767\n",
      "Epoch: 475, Loss: 0.07449258863925934\n",
      "Epoch: 476, Loss: 0.045138176530599594\n",
      "Epoch: 477, Loss: 0.07419465482234955\n",
      "Epoch: 478, Loss: 0.0857749879360199\n",
      "Epoch: 479, Loss: 0.04127153754234314\n",
      "Epoch: 480, Loss: 0.04097844660282135\n",
      "Epoch: 481, Loss: 0.083762988448143\n",
      "Epoch: 482, Loss: 0.04661864787340164\n",
      "Epoch: 483, Loss: 0.08472629636526108\n",
      "Epoch: 484, Loss: 0.07242627441883087\n",
      "Epoch: 485, Loss: 0.08136533200740814\n",
      "Epoch: 486, Loss: 0.09142574667930603\n",
      "Epoch: 487, Loss: 0.08501189947128296\n",
      "Epoch: 488, Loss: 0.08288497477769852\n",
      "Epoch: 489, Loss: 0.08268424868583679\n",
      "Epoch: 490, Loss: 0.09102518856525421\n",
      "Epoch: 491, Loss: 0.07330098748207092\n",
      "Epoch: 492, Loss: 0.044078100472688675\n",
      "Epoch: 493, Loss: 0.09046084433794022\n",
      "Epoch: 494, Loss: 0.07288876175880432\n",
      "Epoch: 495, Loss: 0.08443867415189743\n",
      "Epoch: 496, Loss: 0.08406585454940796\n",
      "Epoch: 497, Loss: 0.08351323753595352\n",
      "Epoch: 498, Loss: 0.08156783133745193\n",
      "Epoch: 499, Loss: 0.08374522626399994\n",
      "Epoch: 500, Loss: 0.08325745910406113\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 500\n",
    "model = model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, targets) in enumerate(train_dataloader):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {losses.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
