{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download labeling from GitHub - https://github.com/tzutalin/labelImg\n",
    "\n",
    "\n",
    "`!pip install pyqt5`\n",
    "\n",
    "`!pip install lxml`\n",
    "\n",
    "\n",
    "Installation guide - https://github.com/heartexlabs/labelImg#installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this command in the terminal\n",
    "\n",
    "`pyrcc5 -o libs/resources.py resources.qrc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models.detection.retinanet import RetinaNet_ResNet50_FPN_Weights\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Number of classes including background\n",
    "num_classes = 91 \n",
    "\n",
    "# Load a pre-trained model\n",
    "model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True, num_classes=num_classes)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Continue with your code...(Origional numbers)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform if transform else transforms.ToTensor()\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root_dir, \"images\"))))\n",
    "        self.labels = list(sorted(os.listdir(os.path.join(root_dir, \"labels\"))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, \"images\", self.imgs[idx])\n",
    "        label_path = os.path.join(self.root_dir, \"labels\", self.labels[idx])\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "        # Apply transformation after getting original size\n",
    "        img = self.transform(img)\n",
    "\n",
    "        # Read YOLO label file\n",
    "        with open(label_path, \"r\") as file:\n",
    "            lines = file.read().splitlines()\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            class_id, x_center, y_center, width, height = map(float, line.split())\n",
    "            labels.append(int(class_id))\n",
    "\n",
    "            x_min = img_width * (x_center - width / 2)\n",
    "            y_min = img_height * (y_center - height / 2)\n",
    "            x_max = img_width * (x_center + width / 2)\n",
    "            y_max = img_height * (y_center + height / 2)\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.tensor(boxes, dtype=torch.float32)\n",
    "        target['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "# Define your own paths here\n",
    "train_dataset = YOLODataset(\"data/train set\")\n",
    "valid_dataset = YOLODataset(\"data/validation set\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    return images, targets\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.43990445137023926\n",
      "Epoch: 2, Loss: 0.38618481159210205\n",
      "Epoch: 3, Loss: 0.20292797684669495\n",
      "Epoch: 4, Loss: 0.20639599859714508\n",
      "Epoch: 5, Loss: 0.16347703337669373\n",
      "Epoch: 6, Loss: 0.1724870204925537\n",
      "Epoch: 7, Loss: 0.12876035273075104\n",
      "Epoch: 8, Loss: 0.098701611161232\n",
      "Epoch: 9, Loss: 0.20843347907066345\n",
      "Epoch: 10, Loss: 0.1403430700302124\n",
      "Epoch: 11, Loss: 0.07947002351284027\n",
      "Epoch: 12, Loss: 0.09271256625652313\n",
      "Epoch: 13, Loss: 0.12376318871974945\n",
      "Epoch: 14, Loss: 0.09589266031980515\n",
      "Epoch: 15, Loss: 0.13618016242980957\n",
      "Epoch: 16, Loss: 0.08114887028932571\n",
      "Epoch: 17, Loss: 0.1088576465845108\n",
      "Epoch: 18, Loss: 0.10441714525222778\n",
      "Epoch: 19, Loss: 0.09452976286411285\n",
      "Epoch: 20, Loss: 0.1113516166806221\n",
      "Epoch: 21, Loss: 0.10474219918251038\n",
      "Epoch: 22, Loss: 0.07956115156412125\n",
      "Epoch: 23, Loss: 0.10429853945970535\n",
      "Epoch: 24, Loss: 0.06037108600139618\n",
      "Epoch: 25, Loss: 0.08999770879745483\n",
      "Epoch: 26, Loss: 0.0896042063832283\n",
      "Epoch: 27, Loss: 0.10325322300195694\n",
      "Epoch: 28, Loss: 0.09524495154619217\n",
      "Epoch: 29, Loss: 0.09455659240484238\n",
      "Epoch: 30, Loss: 0.07118869572877884\n",
      "Epoch: 31, Loss: 0.09349329769611359\n",
      "Epoch: 32, Loss: 0.10457144677639008\n",
      "Epoch: 33, Loss: 0.0866117849946022\n",
      "Epoch: 34, Loss: 0.07641124725341797\n",
      "Epoch: 35, Loss: 0.07673939317464828\n",
      "Epoch: 36, Loss: 0.051496487110853195\n",
      "Epoch: 37, Loss: 0.04753440245985985\n",
      "Epoch: 38, Loss: 0.06083563342690468\n",
      "Epoch: 39, Loss: 0.06720537692308426\n",
      "Epoch: 40, Loss: 0.08794903010129929\n",
      "Epoch: 41, Loss: 0.08519042283296585\n",
      "Epoch: 42, Loss: 0.09167428314685822\n",
      "Epoch: 43, Loss: 0.07553432136774063\n",
      "Epoch: 44, Loss: 0.09326590597629547\n",
      "Epoch: 45, Loss: 0.06848525255918503\n",
      "Epoch: 46, Loss: 0.07262486219406128\n",
      "Epoch: 47, Loss: 0.07572144269943237\n",
      "Epoch: 48, Loss: 0.06349879503250122\n",
      "Epoch: 49, Loss: 0.0767931193113327\n",
      "Epoch: 50, Loss: 0.07012030482292175\n",
      "Epoch: 51, Loss: 0.06241979822516441\n",
      "Epoch: 52, Loss: 0.057208288460969925\n",
      "Epoch: 53, Loss: 0.0727551206946373\n",
      "Epoch: 54, Loss: 0.06903913617134094\n",
      "Epoch: 55, Loss: 0.08109384030103683\n",
      "Epoch: 56, Loss: 0.07580521702766418\n",
      "Epoch: 57, Loss: 0.08037436008453369\n",
      "Epoch: 58, Loss: 0.07310864329338074\n",
      "Epoch: 59, Loss: 0.03762558475136757\n",
      "Epoch: 60, Loss: 0.07254384458065033\n",
      "Epoch: 61, Loss: 0.06356833875179291\n",
      "Epoch: 62, Loss: 0.05543224886059761\n",
      "Epoch: 63, Loss: 0.06432410329580307\n",
      "Epoch: 64, Loss: 0.06203615292906761\n",
      "Epoch: 65, Loss: 0.05873623862862587\n",
      "Epoch: 66, Loss: 0.05806976556777954\n",
      "Epoch: 67, Loss: 0.04745175689458847\n",
      "Epoch: 68, Loss: 0.05110117793083191\n",
      "Epoch: 69, Loss: 0.046534083783626556\n",
      "Epoch: 70, Loss: 0.0554218664765358\n",
      "Epoch: 71, Loss: 0.02745232731103897\n",
      "Epoch: 72, Loss: 0.06003902480006218\n",
      "Epoch: 73, Loss: 0.04853881150484085\n",
      "Epoch: 74, Loss: 0.051033250987529755\n",
      "Epoch: 75, Loss: 0.049774497747421265\n",
      "Epoch: 76, Loss: 0.0511118620634079\n",
      "Epoch: 77, Loss: 0.060470111668109894\n",
      "Epoch: 78, Loss: 0.04392251744866371\n",
      "Epoch: 79, Loss: 0.05933768302202225\n",
      "Epoch: 80, Loss: 0.03519295156002045\n",
      "Epoch: 81, Loss: 0.05828992649912834\n",
      "Epoch: 82, Loss: 0.04792332649230957\n",
      "Epoch: 83, Loss: 0.05564873665571213\n",
      "Epoch: 84, Loss: 0.03107967972755432\n",
      "Epoch: 85, Loss: 0.03175022080540657\n",
      "Epoch: 86, Loss: 0.04757605865597725\n",
      "Epoch: 87, Loss: 0.04920656606554985\n",
      "Epoch: 88, Loss: 0.04067152738571167\n",
      "Epoch: 89, Loss: 0.0741281658411026\n",
      "Epoch: 90, Loss: 0.05528246983885765\n",
      "Epoch: 91, Loss: 0.051072072237730026\n",
      "Epoch: 92, Loss: 0.047108590602874756\n",
      "Epoch: 93, Loss: 0.06105179339647293\n",
      "Epoch: 94, Loss: 0.0503254160284996\n",
      "Epoch: 95, Loss: 0.05653257295489311\n",
      "Epoch: 96, Loss: 0.051488082855939865\n",
      "Epoch: 97, Loss: 0.022550929337739944\n",
      "Epoch: 98, Loss: 0.024928245693445206\n",
      "Epoch: 99, Loss: 0.04620064049959183\n",
      "Epoch: 100, Loss: 0.0372002050280571\n",
      "Epoch: 101, Loss: 0.02503986284136772\n",
      "Epoch: 102, Loss: 0.048656437546014786\n",
      "Epoch: 103, Loss: 0.020129689946770668\n",
      "Epoch: 104, Loss: 0.04709458723664284\n",
      "Epoch: 105, Loss: 0.039905279874801636\n",
      "Epoch: 106, Loss: 0.03902460262179375\n",
      "Epoch: 107, Loss: 0.047077298164367676\n",
      "Epoch: 108, Loss: 0.045308951288461685\n",
      "Epoch: 109, Loss: 0.04987804591655731\n",
      "Epoch: 110, Loss: 0.03862527385354042\n",
      "Epoch: 111, Loss: 0.03021933138370514\n",
      "Epoch: 112, Loss: 0.017034240067005157\n",
      "Epoch: 113, Loss: 0.04427216202020645\n",
      "Epoch: 114, Loss: 0.038115501403808594\n",
      "Epoch: 115, Loss: 0.026269838213920593\n",
      "Epoch: 116, Loss: 0.04247049242258072\n",
      "Epoch: 117, Loss: 0.0502491295337677\n",
      "Epoch: 118, Loss: 0.037161651998758316\n",
      "Epoch: 119, Loss: 0.029987012967467308\n",
      "Epoch: 120, Loss: 0.04381866008043289\n",
      "Epoch: 121, Loss: 0.03437148034572601\n",
      "Epoch: 122, Loss: 0.033794134855270386\n",
      "Epoch: 123, Loss: 0.04341622442007065\n",
      "Epoch: 124, Loss: 0.022310372442007065\n",
      "Epoch: 125, Loss: 0.07684841006994247\n",
      "Epoch: 126, Loss: 0.035133276134729385\n",
      "Epoch: 127, Loss: 0.04179961234331131\n",
      "Epoch: 128, Loss: 0.035181526094675064\n",
      "Epoch: 129, Loss: 0.04605625569820404\n",
      "Epoch: 130, Loss: 0.028951849788427353\n",
      "Epoch: 131, Loss: 0.03315695375204086\n",
      "Epoch: 132, Loss: 0.036365777254104614\n",
      "Epoch: 133, Loss: 0.03866269439458847\n",
      "Epoch: 134, Loss: 0.047545358538627625\n",
      "Epoch: 135, Loss: 0.038031674921512604\n",
      "Epoch: 136, Loss: 0.04353298619389534\n",
      "Epoch: 137, Loss: 0.032616253942251205\n",
      "Epoch: 138, Loss: 0.03004206158220768\n",
      "Epoch: 139, Loss: 0.04332355782389641\n",
      "Epoch: 140, Loss: 0.052299655973911285\n",
      "Epoch: 141, Loss: 0.034356482326984406\n",
      "Epoch: 142, Loss: 0.03862370550632477\n",
      "Epoch: 143, Loss: 0.03802729398012161\n",
      "Epoch: 144, Loss: 0.03138958662748337\n",
      "Epoch: 145, Loss: 0.03196009621024132\n",
      "Epoch: 146, Loss: 0.04213763028383255\n",
      "Epoch: 147, Loss: 0.034339383244514465\n",
      "Epoch: 148, Loss: 0.016255861148238182\n",
      "Epoch: 149, Loss: 0.02720949612557888\n",
      "Epoch: 150, Loss: 0.024711016565561295\n",
      "Epoch: 151, Loss: 0.031762201339006424\n",
      "Epoch: 152, Loss: 0.01563623733818531\n",
      "Epoch: 153, Loss: 0.036657415330410004\n",
      "Epoch: 154, Loss: 0.04139208048582077\n",
      "Epoch: 155, Loss: 0.02301553636789322\n",
      "Epoch: 156, Loss: 0.03346046432852745\n",
      "Epoch: 157, Loss: 0.048588719218969345\n",
      "Epoch: 158, Loss: 0.03818996623158455\n",
      "Epoch: 159, Loss: 0.027353430166840553\n",
      "Epoch: 160, Loss: 0.03267400339245796\n",
      "Epoch: 161, Loss: 0.02902471460402012\n",
      "Epoch: 162, Loss: 0.02522878907620907\n",
      "Epoch: 163, Loss: 0.020304901525378227\n",
      "Epoch: 164, Loss: 0.021991241723299026\n",
      "Epoch: 165, Loss: 0.0348895899951458\n",
      "Epoch: 166, Loss: 0.03217823803424835\n",
      "Epoch: 167, Loss: 0.01695598103106022\n",
      "Epoch: 168, Loss: 0.0169079452753067\n",
      "Epoch: 169, Loss: 0.032816726714372635\n",
      "Epoch: 170, Loss: 0.03634727746248245\n",
      "Epoch: 171, Loss: 0.031125830486416817\n",
      "Epoch: 172, Loss: 0.03549499809741974\n",
      "Epoch: 173, Loss: 0.033202990889549255\n",
      "Epoch: 174, Loss: 0.01833701878786087\n",
      "Epoch: 175, Loss: 0.02890796586871147\n",
      "Epoch: 176, Loss: 0.03352786973118782\n",
      "Epoch: 177, Loss: 0.03056829795241356\n",
      "Epoch: 178, Loss: 0.012577785179018974\n",
      "Epoch: 179, Loss: 0.04303726553916931\n",
      "Epoch: 180, Loss: 0.030512670055031776\n",
      "Epoch: 181, Loss: 0.03944713994860649\n",
      "Epoch: 182, Loss: 0.0378391370177269\n",
      "Epoch: 183, Loss: 0.019033372402191162\n",
      "Epoch: 184, Loss: 0.034103430807590485\n",
      "Epoch: 185, Loss: 0.02547353319823742\n",
      "Epoch: 186, Loss: 0.05783103033900261\n",
      "Epoch: 187, Loss: 0.03368717432022095\n",
      "Epoch: 188, Loss: 0.053259629756212234\n",
      "Epoch: 189, Loss: 0.03105299361050129\n",
      "Epoch: 190, Loss: 0.05254172906279564\n",
      "Epoch: 191, Loss: 0.03934137150645256\n",
      "Epoch: 192, Loss: 0.039123810827732086\n",
      "Epoch: 193, Loss: 0.03496798872947693\n",
      "Epoch: 194, Loss: 0.028574444353580475\n",
      "Epoch: 195, Loss: 0.030951060354709625\n",
      "Epoch: 196, Loss: 0.027192071080207825\n",
      "Epoch: 197, Loss: 0.02994346059858799\n",
      "Epoch: 198, Loss: 0.02069976180791855\n",
      "Epoch: 199, Loss: 0.026284076273441315\n",
      "Epoch: 200, Loss: 0.018431028351187706\n",
      "Epoch: 201, Loss: 0.01983639784157276\n",
      "Epoch: 202, Loss: 0.03597109019756317\n",
      "Epoch: 203, Loss: 0.03236477077007294\n",
      "Epoch: 204, Loss: 0.027991365641355515\n",
      "Epoch: 205, Loss: 0.021490974351763725\n",
      "Epoch: 206, Loss: 0.02231571078300476\n",
      "Epoch: 207, Loss: 0.02075868472456932\n",
      "Epoch: 208, Loss: 0.02846931293606758\n",
      "Epoch: 209, Loss: 0.02303888089954853\n",
      "Epoch: 210, Loss: 0.025816474109888077\n",
      "Epoch: 211, Loss: 0.02882295846939087\n",
      "Epoch: 212, Loss: 0.024190926924347878\n",
      "Epoch: 213, Loss: 0.027779441326856613\n",
      "Epoch: 214, Loss: 0.028112299740314484\n",
      "Epoch: 215, Loss: 0.026766503229737282\n",
      "Epoch: 216, Loss: 0.011188545264303684\n",
      "Epoch: 217, Loss: 0.02582893893122673\n",
      "Epoch: 218, Loss: 0.028298312798142433\n",
      "Epoch: 219, Loss: 0.02584206685423851\n",
      "Epoch: 220, Loss: 0.022080592811107635\n",
      "Epoch: 221, Loss: 0.022649014368653297\n",
      "Epoch: 222, Loss: 0.024447042495012283\n",
      "Epoch: 223, Loss: 0.027890371158719063\n",
      "Epoch: 224, Loss: 0.030693419277668\n",
      "Epoch: 225, Loss: 0.011278580874204636\n",
      "Epoch: 226, Loss: 0.0261948611587286\n",
      "Epoch: 227, Loss: 0.02270396426320076\n",
      "Epoch: 228, Loss: 0.026908619329333305\n",
      "Epoch: 229, Loss: 0.024600129574537277\n",
      "Epoch: 230, Loss: 0.02422037348151207\n",
      "Epoch: 231, Loss: 0.026393236592411995\n",
      "Epoch: 232, Loss: 0.0270652137696743\n",
      "Epoch: 233, Loss: 0.03400097042322159\n",
      "Epoch: 234, Loss: 0.018884649500250816\n",
      "Epoch: 235, Loss: 0.033533673733472824\n",
      "Epoch: 236, Loss: 0.021953852847218513\n",
      "Epoch: 237, Loss: 0.03170829638838768\n",
      "Epoch: 238, Loss: 0.03142229840159416\n",
      "Epoch: 239, Loss: 0.022877132520079613\n",
      "Epoch: 240, Loss: 0.03209321200847626\n",
      "Epoch: 241, Loss: 0.013174382038414478\n",
      "Epoch: 242, Loss: 0.017787393182516098\n",
      "Epoch: 243, Loss: 0.03032655268907547\n",
      "Epoch: 244, Loss: 0.02388676069676876\n",
      "Epoch: 245, Loss: 0.02473149262368679\n",
      "Epoch: 246, Loss: 0.024594372138381004\n",
      "Epoch: 247, Loss: 0.021956751123070717\n",
      "Epoch: 248, Loss: 0.011681175790727139\n",
      "Epoch: 249, Loss: 0.024263255298137665\n",
      "Epoch: 250, Loss: 0.018320385366678238\n",
      "Epoch: 251, Loss: 0.024636289104819298\n",
      "Epoch: 252, Loss: 0.021157996729016304\n",
      "Epoch: 253, Loss: 0.020505448803305626\n",
      "Epoch: 254, Loss: 0.026613600552082062\n",
      "Epoch: 255, Loss: 0.02391773834824562\n",
      "Epoch: 256, Loss: 0.02491040900349617\n",
      "Epoch: 257, Loss: 0.024772662669420242\n",
      "Epoch: 258, Loss: 0.021707186475396156\n",
      "Epoch: 259, Loss: 0.042201168835163116\n",
      "Epoch: 260, Loss: 0.024968793615698814\n",
      "Epoch: 261, Loss: 0.020542588084936142\n",
      "Epoch: 262, Loss: 0.025789156556129456\n",
      "Epoch: 263, Loss: 0.02289644628763199\n",
      "Epoch: 264, Loss: 0.02772093191742897\n",
      "Epoch: 265, Loss: 0.020549695938825607\n",
      "Epoch: 266, Loss: 0.024091647937893867\n",
      "Epoch: 267, Loss: 0.02677113376557827\n",
      "Epoch: 268, Loss: 0.023788515478372574\n",
      "Epoch: 269, Loss: 0.028581175953149796\n",
      "Epoch: 270, Loss: 0.022322162985801697\n",
      "Epoch: 271, Loss: 0.041006192564964294\n",
      "Epoch: 272, Loss: 0.026403101161122322\n",
      "Epoch: 273, Loss: 0.031046388670802116\n",
      "Epoch: 274, Loss: 0.033663105219602585\n",
      "Epoch: 275, Loss: 0.03950434923171997\n",
      "Epoch: 276, Loss: 0.0404801107943058\n",
      "Epoch: 277, Loss: 0.025220118463039398\n",
      "Epoch: 278, Loss: 0.030698155984282494\n",
      "Epoch: 279, Loss: 0.02498745732009411\n",
      "Epoch: 280, Loss: 0.027154207229614258\n",
      "Epoch: 281, Loss: 0.02732648141682148\n",
      "Epoch: 282, Loss: 0.022623401135206223\n",
      "Epoch: 283, Loss: 0.02566608600318432\n",
      "Epoch: 284, Loss: 0.023067770525813103\n",
      "Epoch: 285, Loss: 0.02100871317088604\n",
      "Epoch: 286, Loss: 0.02120688371360302\n",
      "Epoch: 287, Loss: 0.025281080976128578\n",
      "Epoch: 288, Loss: 0.0182475782930851\n",
      "Epoch: 289, Loss: 0.01629348285496235\n",
      "Epoch: 290, Loss: 0.018940534442663193\n",
      "Epoch: 291, Loss: 0.027031728997826576\n",
      "Epoch: 292, Loss: 0.02529723383486271\n",
      "Epoch: 293, Loss: 0.021767612546682358\n",
      "Epoch: 294, Loss: 0.031308647245168686\n",
      "Epoch: 295, Loss: 0.029642872512340546\n",
      "Epoch: 296, Loss: 0.02874600514769554\n",
      "Epoch: 297, Loss: 0.020053643733263016\n",
      "Epoch: 298, Loss: 0.02396722324192524\n",
      "Epoch: 299, Loss: 0.03130393847823143\n",
      "Epoch: 300, Loss: 0.03770764172077179\n",
      "Epoch: 301, Loss: 0.04562243074178696\n",
      "Epoch: 302, Loss: 0.02964673563838005\n",
      "Epoch: 303, Loss: 0.032238926738500595\n",
      "Epoch: 304, Loss: 0.032614707946777344\n",
      "Epoch: 305, Loss: 0.017948752269148827\n",
      "Epoch: 306, Loss: 0.027504591271281242\n",
      "Epoch: 307, Loss: 0.01860775798559189\n",
      "Epoch: 308, Loss: 0.0238022617995739\n",
      "Epoch: 309, Loss: 0.022042861208319664\n",
      "Epoch: 310, Loss: 0.022595014423131943\n",
      "Epoch: 311, Loss: 0.028359292075037956\n",
      "Epoch: 312, Loss: 0.022746045142412186\n",
      "Epoch: 313, Loss: 0.012628216296434402\n",
      "Epoch: 314, Loss: 0.019249631091952324\n",
      "Epoch: 315, Loss: 0.021311352029442787\n",
      "Epoch: 316, Loss: 0.022639894858002663\n",
      "Epoch: 317, Loss: 0.02367429994046688\n",
      "Epoch: 318, Loss: 0.024184100329875946\n",
      "Epoch: 319, Loss: 0.019076520577073097\n",
      "Epoch: 320, Loss: 0.020392771810293198\n",
      "Epoch: 321, Loss: 0.02185513824224472\n",
      "Epoch: 322, Loss: 0.01414458081126213\n",
      "Epoch: 323, Loss: 0.020129220560193062\n",
      "Epoch: 324, Loss: 0.024200478568673134\n",
      "Epoch: 325, Loss: 0.033570606261491776\n",
      "Epoch: 326, Loss: 0.032159462571144104\n",
      "Epoch: 327, Loss: 0.02433832734823227\n",
      "Epoch: 328, Loss: 0.047836411744356155\n",
      "Epoch: 329, Loss: 0.04752286151051521\n",
      "Epoch: 330, Loss: 0.027325771749019623\n",
      "Epoch: 331, Loss: 0.02392515540122986\n",
      "Epoch: 332, Loss: 0.030143197625875473\n",
      "Epoch: 333, Loss: 0.020843548700213432\n",
      "Epoch: 334, Loss: 0.028769196942448616\n",
      "Epoch: 335, Loss: 0.023328568786382675\n",
      "Epoch: 336, Loss: 0.01679571159183979\n",
      "Epoch: 337, Loss: 0.02591407671570778\n",
      "Epoch: 338, Loss: 0.03220362588763237\n",
      "Epoch: 339, Loss: 0.03272216394543648\n",
      "Epoch: 340, Loss: 0.02807878702878952\n",
      "Epoch: 341, Loss: 0.03014323301613331\n",
      "Epoch: 342, Loss: 0.01796756684780121\n",
      "Epoch: 343, Loss: 0.02633647993206978\n",
      "Epoch: 344, Loss: 0.020866919308900833\n",
      "Epoch: 345, Loss: 0.031041206791996956\n",
      "Epoch: 346, Loss: 0.034551531076431274\n",
      "Epoch: 347, Loss: 0.023814570158720016\n",
      "Epoch: 348, Loss: 0.023890137672424316\n",
      "Epoch: 349, Loss: 0.0215169508010149\n",
      "Epoch: 350, Loss: 0.028441961854696274\n",
      "Epoch: 351, Loss: 0.023050693795084953\n",
      "Epoch: 352, Loss: 0.025237251073122025\n",
      "Epoch: 353, Loss: 0.01976495049893856\n",
      "Epoch: 354, Loss: 0.024136481806635857\n",
      "Epoch: 355, Loss: 0.01275575440376997\n",
      "Epoch: 356, Loss: 0.02121550776064396\n",
      "Epoch: 357, Loss: 0.019889336079359055\n",
      "Epoch: 358, Loss: 0.023881055414676666\n",
      "Epoch: 359, Loss: 0.01795782893896103\n",
      "Epoch: 360, Loss: 0.01914350688457489\n",
      "Epoch: 361, Loss: 0.03198152780532837\n",
      "Epoch: 362, Loss: 0.014589397236704826\n",
      "Epoch: 363, Loss: 0.025403302162885666\n",
      "Epoch: 364, Loss: 0.025969216600060463\n",
      "Epoch: 365, Loss: 0.01642654649913311\n",
      "Epoch: 366, Loss: 0.02199537120759487\n",
      "Epoch: 367, Loss: 0.01924128271639347\n",
      "Epoch: 368, Loss: 0.018799375742673874\n",
      "Epoch: 369, Loss: 0.021149007603526115\n",
      "Epoch: 370, Loss: 0.01828085258603096\n",
      "Epoch: 371, Loss: 0.025896744802594185\n",
      "Epoch: 372, Loss: 0.02173813432455063\n",
      "Epoch: 373, Loss: 0.0275474414229393\n",
      "Epoch: 374, Loss: 0.04502618685364723\n",
      "Epoch: 375, Loss: 0.03372165933251381\n",
      "Epoch: 376, Loss: 0.019987311214208603\n",
      "Epoch: 377, Loss: 0.021572645753622055\n",
      "Epoch: 378, Loss: 0.0298630278557539\n",
      "Epoch: 379, Loss: 0.02129533141851425\n",
      "Epoch: 380, Loss: 0.026865240186452866\n",
      "Epoch: 381, Loss: 0.014212069101631641\n",
      "Epoch: 382, Loss: 0.016368743032217026\n",
      "Epoch: 383, Loss: 0.019992923364043236\n",
      "Epoch: 384, Loss: 0.009895158931612968\n",
      "Epoch: 385, Loss: 0.018339814618229866\n",
      "Epoch: 386, Loss: 0.030439937487244606\n",
      "Epoch: 387, Loss: 0.016789576038718224\n",
      "Epoch: 388, Loss: 0.015955287963151932\n",
      "Epoch: 389, Loss: 0.025478284806013107\n",
      "Epoch: 390, Loss: 0.01634586602449417\n",
      "Epoch: 391, Loss: 0.01724999211728573\n",
      "Epoch: 392, Loss: 0.021036317571997643\n",
      "Epoch: 393, Loss: 0.01246996782720089\n",
      "Epoch: 394, Loss: 0.02473921701312065\n",
      "Epoch: 395, Loss: 0.023712338879704475\n",
      "Epoch: 396, Loss: 0.027149733155965805\n",
      "Epoch: 397, Loss: 0.021871298551559448\n",
      "Epoch: 398, Loss: 0.03903844207525253\n",
      "Epoch: 399, Loss: 0.03570576757192612\n",
      "Epoch: 400, Loss: 0.024981675669550896\n",
      "Epoch: 401, Loss: 0.02059059590101242\n",
      "Epoch: 402, Loss: 0.021883001551032066\n",
      "Epoch: 403, Loss: 0.020947879180312157\n",
      "Epoch: 404, Loss: 0.022379087284207344\n",
      "Epoch: 405, Loss: 0.017409823834896088\n",
      "Epoch: 406, Loss: 0.019960012286901474\n",
      "Epoch: 407, Loss: 0.021243488416075706\n",
      "Epoch: 408, Loss: 0.022490685805678368\n",
      "Epoch: 409, Loss: 0.0169608686119318\n",
      "Epoch: 410, Loss: 0.02029355615377426\n",
      "Epoch: 411, Loss: 0.02970544993877411\n",
      "Epoch: 412, Loss: 0.02646384947001934\n",
      "Epoch: 413, Loss: 0.028876831755042076\n",
      "Epoch: 414, Loss: 0.03408876061439514\n",
      "Epoch: 415, Loss: 0.020776527002453804\n",
      "Epoch: 416, Loss: 0.023266293108463287\n",
      "Epoch: 417, Loss: 0.02278760075569153\n",
      "Epoch: 418, Loss: 0.01676626317203045\n",
      "Epoch: 419, Loss: 0.009791978634893894\n",
      "Epoch: 420, Loss: 0.02109026350080967\n",
      "Epoch: 421, Loss: 0.019663527607917786\n",
      "Epoch: 422, Loss: 0.015377639792859554\n",
      "Epoch: 423, Loss: 0.030266068875789642\n",
      "Epoch: 424, Loss: 0.021074648946523666\n",
      "Epoch: 425, Loss: 0.02029816433787346\n",
      "Epoch: 426, Loss: 0.01886237971484661\n",
      "Epoch: 427, Loss: 0.017827389761805534\n",
      "Epoch: 428, Loss: 0.02026216872036457\n",
      "Epoch: 429, Loss: 0.022768639028072357\n",
      "Epoch: 430, Loss: 0.023243382573127747\n",
      "Epoch: 431, Loss: 0.029787536710500717\n",
      "Epoch: 432, Loss: 0.03062206320464611\n",
      "Epoch: 433, Loss: 0.020854027941823006\n",
      "Epoch: 434, Loss: 0.01923912577331066\n",
      "Epoch: 435, Loss: 0.015908274799585342\n",
      "Epoch: 436, Loss: 0.018970942124724388\n",
      "Epoch: 437, Loss: 0.01630837470293045\n",
      "Epoch: 438, Loss: 0.02671235427260399\n",
      "Epoch: 439, Loss: 0.0264957956969738\n",
      "Epoch: 440, Loss: 0.03306986019015312\n",
      "Epoch: 441, Loss: 0.024920344352722168\n",
      "Epoch: 442, Loss: 0.02350902371108532\n",
      "Epoch: 443, Loss: 0.018736563622951508\n",
      "Epoch: 444, Loss: 0.025877080857753754\n",
      "Epoch: 445, Loss: 0.02870633639395237\n",
      "Epoch: 446, Loss: 0.021825645118951797\n",
      "Epoch: 447, Loss: 0.022912079468369484\n",
      "Epoch: 448, Loss: 0.01166024524718523\n",
      "Epoch: 449, Loss: 0.018879154697060585\n",
      "Epoch: 450, Loss: 0.02019372023642063\n",
      "Epoch: 451, Loss: 0.016723209992051125\n",
      "Epoch: 452, Loss: 0.021010762080550194\n",
      "Epoch: 453, Loss: 0.02302122488617897\n",
      "Epoch: 454, Loss: 0.020490895956754684\n",
      "Epoch: 455, Loss: 0.020696550607681274\n",
      "Epoch: 456, Loss: 0.029414823278784752\n",
      "Epoch: 457, Loss: 0.021193336695432663\n",
      "Epoch: 458, Loss: 0.015319782309234142\n",
      "Epoch: 459, Loss: 0.022191451862454414\n",
      "Epoch: 460, Loss: 0.021248536184430122\n",
      "Epoch: 461, Loss: 0.031327519565820694\n",
      "Epoch: 462, Loss: 0.021081294864416122\n",
      "Epoch: 463, Loss: 0.019688045606017113\n",
      "Epoch: 464, Loss: 0.01974453404545784\n",
      "Epoch: 465, Loss: 0.024986330419778824\n",
      "Epoch: 466, Loss: 0.01905731111764908\n",
      "Epoch: 467, Loss: 0.013061330653727055\n",
      "Epoch: 468, Loss: 0.01912345364689827\n",
      "Epoch: 469, Loss: 0.015277772210538387\n",
      "Epoch: 470, Loss: 0.017483502626419067\n",
      "Epoch: 471, Loss: 0.020829493179917336\n",
      "Epoch: 472, Loss: 0.019472848623991013\n",
      "Epoch: 473, Loss: 0.016886821016669273\n",
      "Epoch: 474, Loss: 0.018821706995368004\n",
      "Epoch: 475, Loss: 0.020104985684156418\n",
      "Epoch: 476, Loss: 0.014055212959647179\n",
      "Epoch: 477, Loss: 0.012127876281738281\n",
      "Epoch: 478, Loss: 0.020343316718935966\n",
      "Epoch: 479, Loss: 0.020520592108368874\n",
      "Epoch: 480, Loss: 0.01651880145072937\n",
      "Epoch: 481, Loss: 0.011532174423336983\n",
      "Epoch: 482, Loss: 0.02565646544098854\n",
      "Epoch: 483, Loss: 0.018286654725670815\n",
      "Epoch: 484, Loss: 0.019063759595155716\n",
      "Epoch: 485, Loss: 0.011530529707670212\n",
      "Epoch: 486, Loss: 0.018634803593158722\n",
      "Epoch: 487, Loss: 0.017657939344644547\n",
      "Epoch: 488, Loss: 0.02315264195203781\n",
      "Epoch: 489, Loss: 0.032509706914424896\n",
      "Epoch: 490, Loss: 0.029658246785402298\n",
      "Epoch: 491, Loss: 0.01712428219616413\n",
      "Epoch: 492, Loss: 0.012817279435694218\n",
      "Epoch: 493, Loss: 0.014925802126526833\n",
      "Epoch: 494, Loss: 0.017316540703177452\n",
      "Epoch: 495, Loss: 0.01797623559832573\n",
      "Epoch: 496, Loss: 0.020009858533740044\n",
      "Epoch: 497, Loss: 0.020939694717526436\n",
      "Epoch: 498, Loss: 0.02320188470184803\n",
      "Epoch: 499, Loss: 0.019901297986507416\n",
      "Epoch: 500, Loss: 0.018078362569212914\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 500\n",
    "model = model.to(device)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, targets) in enumerate(train_dataloader):\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch + 1}, Loss: {losses.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
